{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "834ee60f-af36-4803-ac22-6f3663344303",
   "metadata": {},
   "source": [
    "First Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02fc25d-44e7-46ee-82c8-9481fda65b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a3ee39a-ff2c-4008-95c7-226329f34b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedMNIST version: 3.0.2\n",
      "\n",
      "'pneumoniamnist' info dictionary:\n",
      " {'python_class': 'PneumoniaMNIST', 'description': 'The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.', 'url': 'https://zenodo.org/records/10519652/files/pneumoniamnist.npz?download=1', 'MD5': '28209eda62fecd6e6a2d98b1501bb15f', 'url_64': 'https://zenodo.org/records/10519652/files/pneumoniamnist_64.npz?download=1', 'MD5_64': '8f4eceb4ccffa70c672198ea285246c6', 'url_128': 'https://zenodo.org/records/10519652/files/pneumoniamnist_128.npz?download=1', 'MD5_128': '05b46931834c231683c68f40c47b2971', 'url_224': 'https://zenodo.org/records/10519652/files/pneumoniamnist_224.npz?download=1', 'MD5_224': 'd6a3c71de1b945ea11211b03746c1fe1', 'task': 'binary-class', 'label': {'0': 'normal', '1': 'pneumonia'}, 'n_channels': 1, 'n_samples': {'train': 4708, 'val': 524, 'test': 624}, 'license': 'CC BY 4.0'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import medmnist\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from medmnist import INFO, Evaluator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"MedMNIST version:\", medmnist.__version__)\n",
    "print(\"\\n'pneumoniamnist' info dictionary:\\n\", INFO[\"pneumoniamnist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037b6c00-e33b-46ba-b799-27471e7ab932",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def load_npz_data(npz_path):\n",
    "    \"\"\"Load train, val, and test arrays from a given .npz file.\"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Adjust keys if necessary\n",
    "    train_images = data['train_images']\n",
    "    train_labels = data['train_labels']\n",
    "    val_images = data['val_images']\n",
    "    val_labels = data['val_labels']\n",
    "    test_images = data['test_images']\n",
    "    test_labels = data['test_labels']\n",
    "    \n",
    "    return (train_images, train_labels), (val_images, val_labels), (test_images, test_labels)\n",
    "\n",
    "def create_tf_dataset_from_numpy(images, labels, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Convert numpy arrays to a tf.data.Dataset, expanding grayscale images to 3 channels.\"\"\"\n",
    "    if images.ndim == 3:  # shape: (N, H, W)\n",
    "        images = np.expand_dims(images, axis=-1)  # shape: (N, H, W, 1)\n",
    "    if images.shape[-1] == 1:\n",
    "        images = np.tile(images, (1, 1, 1, 3))   # convert grayscale to RGB\n",
    "    \n",
    "    images = images.astype(np.float32) / 255.0\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87735cc6-bee7-41b6-b3be-6f9053141538",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/kaggle/input/tensor-reloaded-multi-task-med-mnist/data\")\n",
    "\n",
    "task_names = [\n",
    "    \"pathmnist\",\n",
    "    \"dermamnist\",\n",
    "    \"octmnist\",\n",
    "    \"pneumoniamnist\",\n",
    "    \"retinamnist\",\n",
    "    \"breastmnist\",\n",
    "    \"bloodmnist\",\n",
    "    \"tissuemnist\",\n",
    "    \"organamnist\",\n",
    "    \"organcmnist\",\n",
    "    \"organsmnist\"\n",
    "]\n",
    "\n",
    "task_to_npz = {task: base_path / f\"{task}.npz\" for task in task_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b72a2a1-2237-42e9-87eb-a33cd800326c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'task_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtask_names\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m => Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_datasets[task])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_datasets[task])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_datasets[task])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'task_names' is not defined"
     ]
    }
   ],
   "source": [
    "for task in task_names:\n",
    "    print(f\"{task} => Train: {len(train_datasets[task])}, \"\n",
    "          f\"Val: {len(val_datasets[task])}, \"\n",
    "          f\"Test: {len(test_datasets[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70c349-91d7-46fb-ab7f-7eb6d5f3cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and verify expected total test samples early\n",
    "expected_total = sum(len(test_datasets[task]) for task in task_names)\n",
    "print(\"Expected total test samples:\", expected_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01abd08-f2af-480c-837f-c6095444eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def create_tf_dataset(medmnist_dataset, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Convert medmnist dataset into a tf.data.Dataset that yields (images, labels).\n",
    "    \"\"\"\n",
    "    x = medmnist_dataset.imgs  # shape: (N, 28, 28) if grayscale; or (N, 28, 28, 3) if as_rgb=True\n",
    "    y = medmnist_dataset.labels.squeeze()  # shape: (N,) or (N,1)\n",
    "\n",
    "    # Convert grayscale data to 3 channels if needed\n",
    "    if len(x.shape) == 3:  # (N, 28, 28)\n",
    "        x = np.expand_dims(x, axis=-1)  # (N, 28, 28, 1)\n",
    "        # Optionally tile to 3 channels\n",
    "        x = np.tile(x, (1, 1, 1, 3))  # (N, 28, 28, 3)\n",
    "\n",
    "    # Create tf.data.Dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    ds = ds.shuffle(buffer_size=len(x)).batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "train_datasets_tf = {task: create_tf_dataset(train_datasets[task], BATCH_SIZE) for task in task_names}\n",
    "val_datasets_tf   = {task: create_tf_dataset(val_datasets[task],   BATCH_SIZE) for task in task_names}\n",
    "test_datasets_tf  = {task: create_tf_dataset(test_datasets[task],  BATCH_SIZE) for task in task_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602ad68-c566-4ae1-90b1-8372267cb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskKerasModel(keras.Model):\n",
    "    def __init__(self, task_names, data_flag_to_info):\n",
    "        super().__init__()\n",
    "        self.task_names = task_names\n",
    "        \n",
    "        # Simple CNN backbone\n",
    "        self.conv1 = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')\n",
    "        self.pool1 = layers.MaxPooling2D()\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
    "        self.pool2 = layers.MaxPooling2D()\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        # Classification heads: a Dense layer for each task\n",
    "        self.heads = {}\n",
    "        for task in task_names:\n",
    "            n_classes = len(data_flag_to_info[task]['label'])\n",
    "            self.heads[task] = layers.Dense(n_classes, name=f'head_{task}')\n",
    "\n",
    "        self.heads = dict(self.heads)  # convert to normal dict for convenience\n",
    "\n",
    "    def call(self, x, task=None, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass. If 'task' is provided, return that task's logits. \n",
    "        Otherwise return a dict of all tasks' logits.\n",
    "        \"\"\"\n",
    "        # Shared feature extraction\n",
    "        x = self.conv1(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        if task is not None:\n",
    "            # Return only the relevant head\n",
    "            return self.heads[task](x)\n",
    "        else:\n",
    "            # Return a dictionary of all heads (not strictly needed for round-robin)\n",
    "            outputs = {}\n",
    "            for t in self.task_names:\n",
    "                outputs[t] = self.heads[t](x)\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289b03b-4458-4352-940d-040f67a9077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskKerasModel(task_names, data_flag_to_info)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# We'll use SparseCategoricalCrossentropy for single-label classification\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab089d53-ce80-4568-9d85-8fef0b78e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 50  # Increased maximum epochs\n",
    "patience = 5       # Number of epochs with no improvement to wait before stopping\n",
    "best_f1 = -np.inf  # Initialize best macro F1 score to negative infinity\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59fbc1d-c7ec-4644-a5fb-63595a97317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(total_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs}...\")\n",
    "    epoch_loss = 0.0\n",
    "    total_batches = 0\n",
    "    \n",
    "    # Training loop over all tasks\n",
    "    for task in task_names:\n",
    "        dataset = train_datasets_tf[task]\n",
    "        \n",
    "        for step, (images, labels) in enumerate(dataset):\n",
    "            images = tf.cast(images, tf.float32)\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(images, task=task, training=True)\n",
    "                loss_value = loss_fn(labels, logits)\n",
    "            \n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            epoch_loss += loss_value.numpy()\n",
    "            total_batches += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / total_batches if total_batches > 0 else 0.0\n",
    "    print(f\"  Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on validation set after each epoch for early stopping\n",
    "    macro_f1_scores = {}\n",
    "    for task in task_names:\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for images, labels in val_datasets_tf[task]:\n",
    "            images = tf.cast(images, tf.float32)\n",
    "            logits = model(images, task=task, training=False)\n",
    "            preds = tf.argmax(logits, axis=1).numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels.numpy())\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        macro_f1_scores[task] = f1\n",
    "    \n",
    "    # Calculate harmonic mean of macro F1 scores across tasks\n",
    "    f1_values = list(macro_f1_scores.values())\n",
    "    if all(f1 > 0 for f1 in f1_values):\n",
    "        harmonic_mean_f1 = len(f1_values) / np.sum(1.0 / np.array(f1_values))\n",
    "    else:\n",
    "        harmonic_mean_f1 = 0.0\n",
    "    \n",
    "    print(f\"  Harmonic Mean Macro F1 on Validation: {harmonic_mean_f1:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if harmonic_mean_f1 > best_f1:\n",
    "        best_f1 = harmonic_mean_f1\n",
    "        patience_counter = 0\n",
    "        # Optionally save the best model weights here\n",
    "        best_weights = model.get_weights()\n",
    "        print(\"  Improved validation macro F1. Saving model weights.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement for {patience_counter} epoch(s).\")\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# After training, restore best model weights if early stopping occurred\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68a2b7-8943-4bcd-8e87-8838b7a1e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValidation Results:\")\n",
    "for task in task_names:\n",
    "    all_preds = []\n",
    "    \n",
    "    # Collect predictions for each batch in the validation set of the current task\n",
    "    for images, labels in val_datasets_tf[task]:\n",
    "        images = tf.cast(images, tf.float32) / 255.0\n",
    "        logits = model(images, task=task, training=False)\n",
    "        preds = tf.argmax(logits, axis=1).numpy()\n",
    "        all_preds.append(preds)\n",
    "\n",
    "    # Concatenate predictions across all batches\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    # Determine number of classes for the current task using the length of the label mapping\n",
    "    n_classes = len(data_flag_to_info[task]['label'])\n",
    "\n",
    "    # Create a one-hot encoded matrix for predictions\n",
    "    y_score = np.zeros((all_preds.shape[0], n_classes))\n",
    "    y_score[np.arange(all_preds.shape[0]), all_preds] = 1\n",
    "\n",
    "    # Initialize the Evaluator for the current task and validation split\n",
    "    evaluator = Evaluator(task, split='val')\n",
    "\n",
    "    # Evaluate using one-hot encoded predictions as scores\n",
    "    metrics = evaluator.evaluate(y_score)\n",
    "\n",
    "    # Extract accuracy and AUC from the returned Metrics object\n",
    "    accuracy = metrics.ACC\n",
    "    auc = metrics.AUC\n",
    "\n",
    "    print(f\"{task} => Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7795c5-8dd8-4758-bd90-765fca81734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store macro F1 scores for each task\n",
    "macro_f1_scores = {}\n",
    "\n",
    "print(\"\\nMacro F1 Scores per Task:\")\n",
    "for task in task_names:\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Collect predictions and true labels for the validation set of the current task\n",
    "    for images, labels in val_datasets_tf[task]:\n",
    "        images = tf.cast(images, tf.float32) / 255.0\n",
    "        logits = model(images, task=task, training=False)\n",
    "        preds = tf.argmax(logits, axis=1).numpy()\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "    # Concatenate predictions and labels across all batches\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Compute macro F1 score for current task\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    macro_f1_scores[task] = f1\n",
    "    print(f\"{task}: {f1:.4f}\")\n",
    "\n",
    "# Compute harmonic mean of all macro F1 scores\n",
    "f1_values = list(macro_f1_scores.values())\n",
    "# Avoid division by zero\n",
    "if all(f1 > 0 for f1 in f1_values):\n",
    "    harmonic_mean = len(f1_values) / np.sum(1.0 / np.array(f1_values))\n",
    "else:\n",
    "    harmonic_mean = 0.0\n",
    "\n",
    "print(f\"\\nHarmonic Mean of Macro F1 Scores: {harmonic_mean:.4f}\")\n",
    "\n",
    "# Plot the macro F1 scores for each task\n",
    "tasks = list(macro_f1_scores.keys())\n",
    "scores = list(macro_f1_scores.values())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(tasks, scores, color='skyblue')\n",
    "plt.axhline(y=harmonic_mean, color='r', linestyle='--', label=f'Harmonic Mean: {harmonic_mean:.4f}')\n",
    "plt.xlabel(\"Task\")\n",
    "plt.ylabel(\"Macro F1 Score\")\n",
    "plt.title(\"Macro F1 Scores per Task with Harmonic Mean\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85661020-93bb-49da-992f-d27a02bce6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_rows = []\n",
    "global_id = 0\n",
    "\n",
    "for task in task_names:\n",
    "    # We'll just iterate again in batch form\n",
    "    # We'll keep a local index for each sample in this task\n",
    "    idx_in_task = 0\n",
    "    \n",
    "    for images, _ in test_datasets_tf[task]:\n",
    "        images = tf.cast(images, tf.float32) / 255.0\n",
    "        logits = model(images, task=task, training=False)\n",
    "        preds = tf.argmax(logits, axis=1).numpy()\n",
    "        \n",
    "        for pred_label in preds:\n",
    "            submission_rows.append([\n",
    "                global_id,\n",
    "                idx_in_task,\n",
    "                task,\n",
    "                int(pred_label)\n",
    "            ])\n",
    "            global_id += 1\n",
    "            idx_in_task += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "submission_df = pd.DataFrame(\n",
    "    submission_rows,\n",
    "    columns=[\"id\", \"id_image_in_task\", \"task_name\", \"label\"]\n",
    ")\n",
    "\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file saved to 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b47961-4141-4641-8133-a6b4991e3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a5536-bac1-4b22-958d-d8e09d7d57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_total = sum(len(test_datasets[task]) for task in task_names)\n",
    "print(\"Expected total test samples:\", expected_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909d321-9d0e-477c-96de-b0c7c588c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in task_names:\n",
    "    print(f\"{task} test samples:\", len(test_datasets[task]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df720460-44bd-4bde-9162-9838e86bd704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
